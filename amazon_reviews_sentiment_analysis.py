# -*- coding: utf-8 -*-
"""Amazon Reviews Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTernVU_-ofMYxNY6H7O2Aw3_mWpB174
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
from textblob import TextBlob
from wordcloud import WordCloud
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
# %matplotlib inline
from plotly.offline import init_notebook_mode , iplot
init_notebook_mode(connected = True)
cf.go_offline();
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from plotly.offline import iplot
from google.colab import drive
drive.mount("/content/drive")

import warnings
warnings.filterwarnings("ignore")
warnings.warn("this won't show")

pd.set_option("display.max_columns" , None)

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/amazon.csv");

df.head(2)

#sort the dataset in descending order
#drop Unnamed:0 column as well
df = df.sort_values(by = "wilson_lower_bound" , ascending = False)
df.drop(columns = ["Unnamed: 0"])
df

#Make a list of columns which has missing values and store in na_columns
#Check number of missing values in each column of na_columns
#Find ratio as a percentage of missing values in each na_columns
#Create a dataframe of na_columns and their respective missing ratios

def analyze_missing_values(df):
    na_columns = [col for col in df.columns if df[col].isnull().sum() > 0]
    n_miss = df[na_columns].isnull().sum().sort_values(ascending=True)
    ratio = (df[na_columns].isnull().sum() / df.shape[0] * 100).sort_values(ascending=True)
    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['Missing Values', 'Ratio'])
    missing_df = pd.DataFrame(missing_df)
    return missing_df

def check_dataframe(df, head=5, tail=5):
    print("SHAPE".center(82, '~'))
    print('Rows: {}'.format(df.shape[0]))
    print('Columns: {}'.format(df.shape[1]))

    print("TYPES".center(82, '~'))
    print(df.dtypes)

    print("".center(82, '~'))

    print(analyze_missing_values(df))

    print("DUPLICATED VALUES".center(83, '~'))
    print(df.duplicated().sum())

    print("QUANTILES".center(82, '~'))
    print(df.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

check_dataframe(df)

#Function for checking unique values in every column of the dataset

#1. Create a dataframe of the unique values in each column
#2. Sort the dataframe in descending order
#3.
def check_unique_values_incolumns(dataframe):
  unique_df = pd.DataFrame({
      'Variable' : dataframe.columns ,
      'Classes' : [
          dataframe[i].nunique()
          for i in dataframe.columns
      ]})
  unique_df = unique_df.sort_values('Classes' , ascending = False)
  unique_df = unique_df.reset_index(drop = True)
  return unique_df

check_unique_values_incolumns(df)

constraints = ['#834D22', '#EBE00C', '#1FEB0C', '#0C92EB', '#EB0CD5']

def categorical_variable_summary(df, column_name):
    fig = make_subplots(rows=1, cols=2,
                        subplot_titles=('Countplot', 'Percentage'),
                        specs=[[{"type": "xy"}, {"type": "domain"}]])

    fig.add_trace(go.Bar(y=df[column_name].value_counts().values.tolist(),
                         x=[str(i) for i in df[column_name].value_counts().index],
                         text=df[column_name].value_counts().values.tolist(),
                         textfont=dict(size=14),
                         name=column_name,
                         textposition='auto',
                         showlegend=False,
                         marker=dict(color=constraints ,
                                     line=dict(color='#DBE6EC' ,
                                               width=1))),
                  row=1, col=1)

    fig.add_trace(go.Pie(labels=df[column_name].value_counts().keys(),
                         values=df[column_name].value_counts().values,
                         textfont=dict(size=18),
                         textposition='auto',
                         showlegend=False,
                         name=column_name,
                         marker=dict(colors=constraints)),
                  row=1, col=2)

    fig.update_layout(title={'text': column_name,
                             'y': 0.9,
                             'x': 0.5,
                             'xanchor': 'center',
                             'yanchor': 'top'},
                      template='plotly_white')

    iplot(fig)


categorical_variable_summary(df,'overall')

#Lets see a random review for example
example_review = df.reviewText[2001]
example_review

#Clean the text (review) from punctuations and numbers using Regex
example_review = re.sub("[^a-zA-Z]",'',example_review)
example_review

#Convert text to lower case because ML algorithm treats a word with a capital letter as a different word from the one with a lower case letter
example_review = example_review.lower().split()
example_review

#Now repeat cell 17 and 18 for the entire dataset
df['reviewText'] = df['reviewText'].apply(lambda review: re.sub("[^a-zA-Z]", ' ', str(review)).lower())
df.head()

#Use vedar sentiment analyzer to check for positive, negative and neutral reviews
sid = SentimentIntensityAnalyzer()
df['compound'] = df['reviewText'].apply(lambda x: sid.polarity_scores(x)['compound'])
df['compound_category'] = df['compound'].apply(lambda x: 'positive' if x >= 0 else 'negative')

df['polarity'] = df['reviewText'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['subjectivity'] = df['reviewText'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
df['textblob_category'] = df.apply(lambda row: 'neutral' if -0.2 < row['polarity'] < 0.2 and 0.4 < row['subjectivity'] < 0.6 else row['compound_category'], axis=1)

print(df[['reviewText', 'compound', 'compound_category', 'polarity', 'subjectivity', 'textblob_category']])

#Print the dataset to check if the necessary columns are added or not
df.head()

#Make pie chart and bar chart for the number of positive, negative and neutral reviews
sentiment_counts = df['textblob_category'].value_counts()

#Pie chart
plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=['#1FEB0C', '#EB0CD5', '#EBE00C'])
plt.title('Distribution of Sentiments in Reviews (TextBlob)')
plt.show()

#Bar chart
plt.figure(figsize=(10, 6))
sentiment_counts.plot(kind='bar', color=['#1FEB0C', '#EB0CD5', '#EBE00C'])
plt.title('Number of Reviews per Sentiment (TextBlob)')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.show()

